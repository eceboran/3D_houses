{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5b293eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import requests as req\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import rasterio as rs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee30a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs for the DSM and DTM data (with 1 meter resolution) provided by Geopunt Flanders\n",
    "dataset_url = {}\n",
    "dataset_url[\"DSM\"] = (\"https://www.geopunt.be/download?container=dhm-vlaanderen-ii-dsm-raster-1m\"\n",
    "                      \"&title=Digitaal%20Hoogtemodel%20Vlaanderen%20II,%20DSM,%20raster,%201m\")\n",
    "dataset_url[\"DTM\"] = (\"https://www.geopunt.be/download?container=dhm-vlaanderen-ii-dtm-raster-1m\"\n",
    "                      \"&title=Digitaal%20Hoogtemodel%20Vlaanderen%20II,%20DTM,%20raster,%201m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa4b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for the raw and processed metadata csv files\n",
    "metadata_folder_path = os.path.join(os.getcwd(), 'data', 'metadata')\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(metadata_folder_path):\n",
    "    os.makedirs(metadata_folder_path)    \n",
    "# Initialize csv file paths\n",
    "metadata_raw_file_path = {}\n",
    "metadata_processed_file_path = {}\n",
    "for data_type in dataset_url.keys():\n",
    "    # Raw files\n",
    "    metadata_raw_file_path[data_type] = os.path.join(metadata_folder_path, data_type + \n",
    "                                                     '_GeoTIFF_1m_metadata_raw.csv')\n",
    "    # Processed files\n",
    "    metadata_processed_file_path[data_type] = os.path.join(metadata_folder_path, data_type + \n",
    "                                                           '_GeoTIFF_1m_metadata_processed.csv')\n",
    "\n",
    "# Preprocessed and merge csv file path\n",
    "metadata_processed_merged_file_path = os.path.join(metadata_folder_path, \n",
    "                                                   'GeoTIFF_1m_metadata_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2289d510",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Links to the zip files that make up the datasets\n",
    "dataset_zip_file_links = {}\n",
    "for data_type in dataset_url.keys():  # Get the links from both datasets\n",
    "    # Initialize the list for the links\n",
    "    dataset_zip_file_links[data_type] = []\n",
    "    try:\n",
    "        # Create a request to the URL\n",
    "        r = req.get(dataset_url[data_type])\n",
    "        soup = bs(r.content, \"html\")\n",
    "        # Find the element that contains the downloadable elements\n",
    "        download_elem = soup.find_all(\"div\", attrs={\"class\": \"downloadfileblock\"})[0]\n",
    "        # Elements for the links to the zip files\n",
    "        link_elems = download_elem.find_all(\"a\")\n",
    "        for link_elem in link_elems:\n",
    "            # Get the hypertext reference of the element for a single zip file link\n",
    "            dataset_zip_file_links[data_type].append(link_elem.get(\"href\"))\n",
    "    except:\n",
    "        print('Please enter the correct URLs for the datasets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26ec5a9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect metadata from online GeoTIFF files in a Pandas DataFrame or load existing DataFrame\n",
    "create_metadata_conditional = False\n",
    "if create_metadata_conditional:  # if True, create csv files with metadata\n",
    "    # Start the timer\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    for data_type in dataset_url.keys():  # for both datasets\n",
    "        # Initialize the DataFrame\n",
    "        metadata_df = pd.DataFrame()\n",
    "\n",
    "        # Access each online GeoTIFF file\n",
    "        for file_no, link in enumerate(dataset_zip_file_links[data_type]):    \n",
    "            # Get the zip file name from the link\n",
    "            m = re.search(\"/(?P<zip_file_name>[\\w]+).zip$\", link)\n",
    "            zip_file_name = m.group(\"zip_file_name\")\n",
    "\n",
    "            # Create the link for the online TIFF file\n",
    "            tif_request = \"zip+\" + link + \"!/GeoTIFF/\" + zip_file_name + \".tif\"\n",
    "\n",
    "            # Access the file to get metadata\n",
    "            with rs.open(tif_request) as src:\n",
    "                profile = src.profile\n",
    "\n",
    "            # Collect metadata\n",
    "            df_row = {}\n",
    "            df_row['file_number'] = file_no+1\n",
    "            df_row['file_name'] = zip_file_name + \".zip\"\n",
    "            df_row['file_link'] = tif_request\n",
    "            df_row['coord_system'] = str(profile['crs'])\n",
    "            df_row['bottom'] = src.bounds.bottom\n",
    "            df_row['top'] = src.bounds.top\n",
    "            df_row['left'] = src.bounds.left\n",
    "            df_row['right'] = src.bounds.right\n",
    "            df_row['height'] = profile['height']\n",
    "            df_row['width'] = profile['width']\n",
    "            \n",
    "            # Save metadata in the DataFrame\n",
    "            metadata_df = metadata_df.append(df_row, ignore_index=True)\n",
    "\n",
    "            # Display execution time\n",
    "            stop_time = timeit.default_timer()  # stop the timer\n",
    "            # print(f\"{data_type} file {file_no} accessed. Execution time: {stop_time-start_time:.2f}\")\n",
    "\n",
    "        # Save raw metadata to a csv file\n",
    "        metadata_df.to_csv(metadata_raw_file_path[data_type], sep=',', index=False)\n",
    "        \n",
    "        # Total execution time\n",
    "        stop_time = timeit.default_timer()\n",
    "        print(f\"All files accessed. Execution time: {stop_time-start_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d967856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw metadata from csv file\n",
    "metadata_df_raw_loaded = {}\n",
    "for data_type in dataset_url.keys():  # for both datasets\n",
    "    metadata_df_raw_loaded[data_type] = pd.read_csv(metadata_raw_file_path[data_type], sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d90a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process raw metadata\n",
    "def clean_metadata(df_in):\n",
    "    \"\"\"\n",
    "    Cleans the GeoTIFF metadata DataFrame\n",
    "    :param df_in: A Pandas DataFrame with metadata\n",
    "    :return: A cleaned DataFrame\n",
    "    \"\"\"\n",
    "    df_out = df_in\n",
    "    # Change data types for the file_number, height and width to unsigned integers\n",
    "    df_out = df_out.astype({'file_number': 'uint8', 'height': 'uint32', 'width': 'uint32'})\n",
    "    # Set file_number as the index\n",
    "    df_out = df_out.set_index('file_number', drop=True)\n",
    "    # Change the order of the columns\n",
    "    columns = ['file_name', 'file_link', 'coord_system', 'height', 'width', \n",
    "               'bottom', 'top', 'left', 'right']\n",
    "    df_out = df_out[columns]\n",
    "    return df_out\n",
    "\n",
    "metadata_df_processed = {}\n",
    "for data_type in dataset_url.keys():  # for both datasets\n",
    "    metadata_df_processed[data_type] = clean_metadata(metadata_df_raw_loaded[data_type])\n",
    "    \n",
    "# Save processed metadata to a csv file\n",
    "for data_type in dataset_url.keys():  # for both datasets\n",
    "    metadata_df_processed[data_type].to_csv(metadata_processed_file_path[data_type], sep=',', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a0b4b23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compare the contents of metadata for DSM and DTM files\n",
    "# Check if columns other than file_name and file_link are equal\n",
    "df_DSM = metadata_df_processed['DSM']\n",
    "df_DTM = metadata_df_processed['DTM']\n",
    "datasets_match_conditional = df_DSM.iloc[:, 2:].equals(df_DTM.iloc[:, 2:])\n",
    "\n",
    "# Rename columns file_name and file_link\n",
    "df_DSM = df_DSM.rename(columns={'file_name': 'dsm_file_name'})\n",
    "df_DSM = df_DSM.rename(columns={'file_link': 'dsm_file_link'})\n",
    "df_DTM = df_DTM.rename(columns={'file_name': 'dtm_file_name'})\n",
    "df_DTM = df_DTM.rename(columns={'file_link': 'dtm_file_link'})\n",
    "\n",
    "# Merge the DataFrames\n",
    "df_merged = pd.concat([df_DSM, df_DTM.iloc[:, :2]], axis=1, join=\"inner\")\n",
    "\n",
    "# Change column order\n",
    "columns = df_merged.columns.tolist()\n",
    "columns = columns[:2] + columns[9:11] + columns[2:9]\n",
    "df_merged = df_merged[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8235b7e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save processed and merged metadata to a csv file\n",
    "df_merged.to_csv(metadata_processed_merged_file_path, sep=',', header=True, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
